{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPStNNkK6nrX+w4EmsUByHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mido-sheikh/CSE616/blob/main/CaesarCipher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jUP58FORtkJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ac0f34-8e69-482c-d7f3-4fcda6ad12f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 101, 1)]          0         \n",
            "                                                                 \n",
            " Layer1 (GRU)                (None, 101, 64)           12864     \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, 101, 32)          2080      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,944\n",
            "Trainable params: 14,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "250/250 [==============================] - 14s 47ms/step - loss: 1.5508 - accuracy: 0.5732 - val_loss: 0.9578 - val_accuracy: 0.7299\n",
            "Epoch 2/4\n",
            "250/250 [==============================] - 12s 46ms/step - loss: 0.7425 - accuracy: 0.8053 - val_loss: 0.5705 - val_accuracy: 0.8639\n",
            "Epoch 3/4\n",
            "250/250 [==============================] - 11s 45ms/step - loss: 0.4510 - accuracy: 0.8966 - val_loss: 0.3609 - val_accuracy: 0.9161\n",
            "Epoch 4/4\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.2953 - accuracy: 0.9362 - val_loss: 0.2435 - val_accuracy: 0.9487\n",
            "t h e   l i m e   i s   b e r   l e a s t   l i k e d   f r u i t   ,   b u t   t h e   b a n a n a   i s   m b   l e a s t   l i k e d   . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Activation\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "import tensorflow\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Load dataset\n",
        "    \"\"\"\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data.split('\\n')\n",
        "\n",
        "def tokenize(x):\n",
        "    \"\"\"\n",
        "    Tokenize x\n",
        "    :param x: List of sentences/strings to be tokenized\n",
        "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x) \n",
        "    \"\"\"\n",
        "    x_tk = Tokenizer(char_level=True)\n",
        "    # because input is text, not sequence (list of integer tokens)\n",
        "    x_tk.fit_on_texts(x)\n",
        "    return x_tk.texts_to_sequences(x), x_tk\n",
        "\n",
        "def pad(x, length=None):\n",
        "    \"\"\"\n",
        "    Pad x\n",
        "    :param x: List of sequences.\n",
        "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
        "    :return: Padded numpy array of sequences\n",
        "    \"\"\"\n",
        "    # Find the length of the longest string in the dataset.\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    # Then, pass it to pad_sentences as the maxlen parameter \n",
        "    return pad_sequences(x, maxlen=length, padding=\"post\", truncating=\"post\",)\n",
        "\n",
        "def preprocess(x, y):\n",
        "    \"\"\"\n",
        "    Preprocess x and y\n",
        "    :param x: Feature List of sentences\n",
        "    :param y: Label List of sentences\n",
        "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
        "    \"\"\"\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "def simple_model(input_shape, output_sequence_length, code_vocab_size, plaintext_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a basic RNN on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param code_vocab_size: Number of unique code characters in the dataset\n",
        "    :param plaintext_vocab_size: Number of unique plaintext characters in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"    \n",
        "    x = Input(shape=input_shape[1:])   # shape(none,54,1) ie   \n",
        "    # output must be batchsize x timesteps x units\n",
        "    seq = GRU(units= 64, return_sequences = True, activation=\"tanh\", name='Layer1')(x)\n",
        "    output = TimeDistributed(Dense(units = plaintext_vocab_size, activation='softmax', name='Layer2'))(seq)\n",
        "    model = Model(inputs = x, outputs = output)   \n",
        "    model.compile(optimizer='adam', loss= sparse_categorical_crossentropy, metrics=['accuracy'])  \n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def logits_to_text(logits, tokenizer):\n",
        "    \"\"\"\n",
        "    Turn logits from a neural network into text using the tokenizer\n",
        "    :param logits: Logits from a neural network\n",
        "    :param tokenizer: Keras Tokenizer fit on the labels\n",
        "    :return: String that represents the text of the logits\n",
        "    \"\"\"\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "\n",
        "codes = load_data('/content/sample_data/cipher.txt')\n",
        "plaintext = load_data('/content/sample_data/plaintext.txt')\n",
        "preproc_code_sentences, preproc_plaintext_sentences, code_tokenizer, plaintext_tokenizer = preprocess(codes, plaintext)\n",
        "\n",
        "# Reshaping the input to work with a basic RNN\n",
        "tmp_x = pad(preproc_code_sentences, preproc_plaintext_sentences.shape[1]) # pad code sequences with maxlen 54: shape=10001x54\n",
        "tmp_x = tmp_x.reshape((-1, preproc_plaintext_sentences.shape[-2], 1))     # reshape padded code seq in 10001 x 54 x 1\n",
        "simple_rnn_model = simple_model(tmp_x.shape,preproc_plaintext_sentences.shape[1],len(code_tokenizer.word_index)+1,len(plaintext_tokenizer.word_index)+1)\n",
        "simple_rnn_model.fit(tmp_x, preproc_plaintext_sentences, batch_size=32, epochs=4, validation_split=0.2)\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], plaintext_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wsqQgYTZzJ8S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}